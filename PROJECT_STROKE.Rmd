---
title: "Untitled"
output: html_document
date: "2024-09-05"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##library loading
```{r 1}
library(caret)
library(nnet)
library(pROC)
library(gmodels)
library(dplyr) 
library(e1071)
library(xgboost)
```

##importing data into 
```{r 2}
StrokePred <- read.csv("C:/Users/katta/Downloads/healthcare-dataset-stroke-data cleaned.csv")
```

##droping ununcessary columns
```{r 3}
StrokePred[,c('work_type','Residence_type','smoking_status','id')] <- list(NULL)
StrokePred
```


##spliting data into train-validation and test 
```{r 4}
set.seed(1234)
#spliting 17% into test dataset
sample <- sample.int(n = nrow(StrokePred), size = nrow(StrokePred)*0.17, replace = F)
strokepred_test <- StrokePred[sample, ] ##Yields held-out test dataset
strokepred_trainvalidation <- StrokePred[-sample, ]
```

```{r}
library(imbalance)
imbalanceRatio(strokepred_trainvalidation, classAttr = 'stroke')
```


```{r}
library(imbalance)
positives <- mwmote(strokepred_trainvalidation,  classAttr = 'stroke', numInstances = 1500)
strokepred_balanced <- rbind(strokepred_trainvalidation, positives)
table(strokepred_balanced$stroke)
imbalanceRatio(strokepred_balanced, classAttr = 'stroke')
```

# Distribution of age by gender
```{r}
ggplot(StrokePred, aes(x = age, fill = Is_Female)) +
  geom_density(alpha = 0.5, color = "#103846") +
  labs(title = "Distribution of patients' gender",
       color = "#1D4B5B") +
  theme_minimal() +
  theme(panel.grid = element_blank())  # Remove gridlines
```


```{r}
ggplot(data = StrokePred, aes(x = Is_Female, fill = factor(stroke))) +
  geom_bar(position = "dodge") +  # Stacked bar based on Stroke variable
   labs(title = "Gender of patient", x = "Is_Female", color = "#1D4B5B") +
  scale_fill_discrete(name = "Stroke", labels = c("No Stroke", "Stroke")) +  # Legend labels
  theme_minimal() +
  theme(panel.grid = element_blank(),  # Remove gridlines
        legend.position = "bottom")
```

##correlation chart
```{r}
install.packages("corrplot")
library(corrplot)
corr_data<-cor(strokepred_balanced)
corrplot(corr_data,method = 'color')
```


##spliting data into train and validation 
```{r 5}
set.seed(1234)
#spliting 17% into validation dataset
sample <- sample.int(n = nrow(strokepred_balanced), size = nrow(strokepred_balanced)*0.17, replace = F)
strokepred_validation <- strokepred_balanced[sample, ] ##Yields validation dataset
strokepred_train <- strokepred_balanced[-sample, ]
```

#checking number of row for each dataset
```{r}
nrow(strokepred_train)
nrow(strokepred_validation)
nrow(strokepred_test)
```


```{r}
strokepred_train_scaled <- scale(strokepred_train[,-ncol(strokepred_train)])
strokepred_validation_scaled <- scale(strokepred_validation[,-ncol(strokepred_validation)])
strokepred_test_scaled <- scale(strokepred_test[,-ncol(strokepred_test)])
```


```{r}
stroke_column <- strokepred_train$stroke
strokepred_train_scaled_xg <- cbind(strokepred_train_scaled, stroke_column)
stroke_column <- strokepred_validation$stroke
strokepred_validation_scaled_xg <- cbind(strokepred_validation_scaled, stroke_column)
stroke_column <- strokepred_test$stroke
strokepred_test_scaled_xg <- cbind(strokepred_test_scaled, stroke_column)
```

##feature selection
```{r}
control <- rfeControl(functions = lmFuncs, # linear regression
                      method = "repeatedcv", # repeated cv
                      repeats = 5, # number of repeats
                      number = 20) # number of folds
```
## Run recursive feature elimination (RFE)
```{r}
result_rfe1 <- rfe(x = strokepred_train_scaled, 
                   y = strokepred_train$stroke, 
                   sizes = c(1:14),
                   rfeControl = control)


result_rfe1

# Print the selected features
predictors(result_rfe1)

```
##Running Logestic Regression Model
```{r }
logistic_regression_model <- glm(stroke ~., data=strokepred_train, family="binomial") ##Or, can use all predictors except one using the ~ . -EXCLUDEDVARIABLE notation
summary(logistic_regression_model) ##Outputs summary of model & coefficients
```

```{r }
#predicting for training data set
TRAINING_PREDICTIONS <- predict(logistic_regression_model, newdata=strokepred_train,type="response")
strokepred_train$LOGIT_PRED = TRAINING_PREDICTIONS

#predicting for validation data set
VALIDATION_PREDICTIONS <- predict(logistic_regression_model, newdata=strokepred_validation,type="response")
strokepred_validation$LOGIT_PRED = VALIDATION_PREDICTIONS

#predicting for testing data set
TEST_PREDICTIONS <- predict(logistic_regression_model, newdata=strokepred_test,type="response")
strokepred_test$LOGIT_PRED = TEST_PREDICTIONS
```


```{r }
##8. Evaluate validation & test predictions
postResample(pred = VALIDATION_PREDICTIONS, obs =
strokepred_validation$stroke)
postResample(pred = TEST_PREDICTIONS, obs = strokepred_test$stroke)
```

```{r}
#training
myroc <- roc(strokepred_train$stroke, strokepred_train$LOGIT_PRED)
auc(myroc) ##Print out AUC of training

#validation
myroc <- roc(strokepred_validation$stroke, strokepred_validation$LOGIT_PRED)
auc(myroc) ##Print out AUC of validation

#test
myroc <- roc(strokepred_test$stroke, strokepred_test$LOGIT_PRED)
auc(myroc) ##Print out AUC of test
```

```{r}
strokepred_validation <- strokepred_validation %>% mutate(LOGIT_CLASSIFICATION = 1*(LOGIT_PRED >= 0.5))

strokepred_test <- strokepred_test %>% mutate(LOGIT_CLASSIFICATION = 1*(LOGIT_PRED >= 0.5))
```

```{r}
validation_performance <- confusionMatrix(data=as.factor(strokepred_validation$LOGIT_CLASSIFICATION), reference = as.factor(strokepred_validation$stroke),positive="1") ##Generate confusion matrix (based on probability cutoff)
validation_performance
```

```{r}
test_performance <- confusionMatrix(data=as.factor(strokepred_test$LOGIT_CLASSIFICATION), reference = as.factor(strokepred_test$stroke),positive="1") ##Generate confusion matrix (based on probability cutoff)
test_performance
```



##Running a SVM model
```{r}

model <- svm(strokepred_train_scaled_xg[, -ncol(strokepred_train_scaled_xg)], 
             kernel = "radial", cost = 1000, gamma = 0.1)  
```


```{r}
# Predict on validation data
validation_pred <- predict(model, strokepred_validation_scaled_xg[, -ncol(strokepred_validation_scaled_xg)])



# Predict on test data
test_pred <- predict(model, strokepred_test_scaled_xg[, -ncol(strokepred_test_scaled_xg)])

```


```{r}
validation_pred_numeric <- ifelse(validation_pred == TRUE, 1, 0)
test_pred_numeric <- ifelse(test_pred == TRUE, 1, 0)

```

```{r}
# Validation Confusion Matrix


strokepred_validation_scaled$preds = validation_pred
strokepred_validation_scaled$preds <- as.integer(as.logical(strokepred_validation_scaled$preds))
# Generate confusion matrix

validation_pred_factor <- factor(strokepred_validation_scaled$preds, levels = c(0, 1))

# Assuming true labels are numeric (0/1)
strokepred_validation$stroke_factor <- factor(strokepred_validation$stroke, levels = c(0, 1))

# Generate confusion matrix
validation_performance <- confusionMatrix(data = validation_pred_factor, reference = strokepred_validation$stroke_factor, positive = "1")
print(validation_performance)

```

```{r}
# Test Confusion Matrix


strokepred_test_scaled$preds = test_pred
strokepred_test_scaled$preds <- as.integer(as.logical(strokepred_test_scaled$preds))
# Generate confusion matrix

test_pred_factor <- factor(strokepred_test_scaled$preds, levels = c(0, 1))

# Assuming true labels are numeric (0/1)
strokepred_test$stroke_factor <- factor(strokepred_test$stroke, levels = c(0, 1))

# Generate confusion matrix
test_performance <- confusionMatrix(data = test_pred_factor, reference = strokepred_test$stroke_factor, positive = "1")
print(test_performance)

```

##Running XGBoost Model
```{r}

# Create DMatrix objects for XGBoost

dtrain <- xgb.DMatrix(data = as.matrix(strokepred_train_scaled_xg[, -ncol(strokepred_train_scaled_xg)]), label = strokepred_train_scaled_xg[, ncol(strokepred_train_scaled_xg)])
dvalidation <- xgb.DMatrix(data = as.matrix(strokepred_validation_scaled_xg[, -ncol(strokepred_validation_scaled_xg)]), label = strokepred_validation_scaled_xg[, ncol(strokepred_validation_scaled_xg)])
dtest <- xgb.DMatrix(data = as.matrix(strokepred_test_scaled_xg[, -ncol(strokepred_test_scaled_xg)]), label = strokepred_test_scaled_xg[, ncol(strokepred_test_scaled_xg)])
```

```{r}
params <- list(
  booster = "gbtree",
  objective = "binary:logistic",  # For classification
  eval_metric = "error",  # Evaluation metric
  max_depth = 3,
  eta = 0.1,
  nrounds = 100
)

model <- xgboost(params = params, data = dtrain, nrounds = params$nrounds)

# Make predictions on validation and test data
validation_pred <- predict(model, dvalidation)
test_pred <- predict(model, dtest)

```


```{r}
thresholds <- seq(0.1, 0.9, by = 0.1)
best_threshold <- NULL
best_f1_score <- 0

for (threshold in thresholds) {
  validation_pred_numeric <- ifelse(validation_pred > threshold, 1, 0)
  validation_cm <- table(actual = strokepred_validation_scaled_xg[, ncol(strokepred_validation_scaled_xg)], predicted = validation_pred_numeric)
  validation_precision <- sum(validation_cm[1, 1]) / sum(validation_cm[, 1])
  validation_recall <- sum(validation_cm[2, 2]) / sum(validation_cm[2, ])
  validation_f1_score <- 2 * validation_precision * validation_recall / (validation_precision + validation_recall)

  if (validation_f1_score > best_f1_score) {
    best_threshold <- threshold
    best_f1_score <- validation_f1_score
  }
}

cat("Best threshold:", best_threshold, "\n")
```

```{r}
# ... (rest of your code)

# Apply the optimal threshold to the test predictions
test_pred_numeric <- ifelse(test_pred > 0.1, 1, 0)

# Calculate the confusion matrix and other metrics
test_cm <- table(actual = strokepred_test_scaled_xg[, ncol(strokepred_test_scaled_xg)], predicted = test_pred_numeric)
test_accuracy <- sum(diag(test_cm)) / sum(test_cm)
test_recall <- sum(test_cm[2, 2]) / sum(test_cm[2, ])
test_precision <- sum(test_cm[1, 1]) / sum(test_cm[, 1])
test_f1_score <- 2 * test_precision * test_recall / (test_precision + test_recall)

# Print results
cat("Test accuracy:", test_accuracy, "\n")
cat("Test recall:", test_recall, "\n")
cat("Test precision:", test_precision, "\n")
cat("Test F1-score:", test_f1_score, "\n")
```



